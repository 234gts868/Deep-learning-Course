{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Mulitclass Classification\n",
    "\n",
    "-  <b> Multiclass classification</b> means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time. \n",
    "- In our case, We use [Cover type dataset]( https://archive.ics.uci.edu/ml/datasets/covertype) and  this dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest. There are over half a million measurements total! This dataset includes information on tree type, shadow coverage, distance to nearby landmarks (roads etcetera), soil type, and local topography.\n",
    "- <b>Our purpose is to predict  forest cover type.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covertype Data Set\n",
    "The dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/covertype)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Data information\n",
    "\n",
    "#### FEUATURES\n",
    "  The order of this listing corresponds to the order of numerals along the rows of the database.\n",
    "- Elevation = 海拔（以米為單位）\n",
    "- Aspect = 以方位角表示的縱橫比\n",
    "- Slope = 坡度\n",
    "- Horizontal_Distance_To_Hydrology = 到水的水平距離\n",
    "- Vertical_Distance_To_Hydrology = 到水的垂直距離\n",
    "- Horizontal_Distance_To_Roadways = 到最近道路的距離\n",
    "- Hillshade_9am = 山丘的陰影在早上9點的投影長度 （0~255)\n",
    "- Hillshade_Noon = 山丘的陰影在中午的投影長度 （0~255)\n",
    "- Hillshade_3pm =山丘的陰影在下午3點的投影長度 （0~255)\n",
    "- Horizontal_Distance_To_Fire_Point = 到最近野火的距離\n",
    "- Wilderness_Area = 荒野地區名稱 (4個二進制列，0 =不存在或1 =存在）\n",
    "    - Wilderness_Area1 = Rawah Wilderness Area\n",
    "    - Wilderness_Area2 = Neota Wilderness Area\n",
    "    - Wilderness_Area3 = Comanche Peak Wilderness Area\n",
    "    - Wilderness_Area4 = Cache la Poudre Wilderness Area\n",
    "- Soil_Type1 to Soil_Type40 = 土壤種類名稱 (40個二進制列，0 =不存在或1 =存在)\n",
    "\n",
    "#### CLASSES\n",
    "- Cover_Type(7個類型，整數1至7）= 森林覆蓋類型指定\n",
    "    - Spruce/Fir\n",
    "    - Lodgepole Pine\n",
    "    - Ponderosa Pine\n",
    "    - Cottonwood/Willow\n",
    "    - Aspen\n",
    "    - Douglas-fir\n",
    "    - Krummholz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and show data dimension\n",
    "We first load the dataset CoverType`covtype.csv`  , and show its dimensions.\n",
    "\n",
    "\n",
    "We can know how many  features and instances in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('covtype.csv')\n",
    "\n",
    "### START CODE HERE ### \n",
    "data_num = \n",
    "feature_num = \n",
    "### END CODE HERE ###\n",
    "\n",
    "print('Data Dimension:')\n",
    "print('Number of data:',data_num)\n",
    "print('Number of features:', feature_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the feature name\n",
    " View the feature name in this dataset and we can more understand these features we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature Names')\n",
    "### START CODE HERE ### \n",
    "print(YOUR CODE HERE) ## 查看資歷的欄位名稱  hint : columns\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the data information\n",
    " Now let us understand the data type of each features.\n",
    " \n",
    " We can use `.info()` and this method shows information about  our data  including the index dtype and column dtypes, non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Information')\n",
    "### START CODE HERE ### \n",
    "print(YOUR CODE HERE) ## 查看資料有幾筆資料、每個欄位的資料型別是什麼 hint : info\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if columns are null or not\n",
    "NULL values are not the same as data values. A NULL value is basically an undefined values.  NULL values cause several problems, In our case, it may lead to bad predicitons.\n",
    " \n",
    "Let's check for missing values  in our dataset or not.\n",
    "\n",
    "\n",
    "hint: We can use function` isnull()` to check every value  is null or not. and use function `sum` to count every column how many null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "    #確認各欄位沒有缺失值 hint : isnull() ,sum()\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Convert  Cover_Type data   into dummies\n",
    "\n",
    "\n",
    "According the above cells, we can find the column `Cover_Type` is `integer`, and  the numbers (from 1 to 7) in column actually are used to represent the forest cover type. \n",
    "\n",
    "\n",
    "- There are 7 categories in the coumn `Cover type` and the numbers represent the  the forest cover types. \n",
    "\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz\n",
    "\n",
    "However, we can see that there is no such thing that forest cover type `Ponderosa Pine`(the correspone number is 3)  is higher than forest cover type `Spruce/Fir` (the correspone number is 1) and   forest cover type `Lodgepole Pine`(the correspone number is 2) is less than forest cover type `Aspen`(the correspone number is 5).\n",
    "\n",
    "To represent the categories varible `Cover_type` , we can make a separate column, or variable, for each category. These columns will each showThese columns will each show whether each category was a foreset cover type; if  this instance has `1` in the coumn `Cover_Type_1`, `Spruce/Fir` is its foreset cover type, but if it have a `0`, `Spruce/Fir` did not make the cut. the cover types. 0 indicates non existent while 1 indicates existent. The same goes for each of the dummy variables, as they are called. \n",
    "\n",
    "Let us  use the dummy variables to represent the `Cover_type`. \n",
    "\n",
    "We use [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function in pandas to do this, but input in this function  is categor varible. So, we first convert the integer varible `Cover_type` into categorical varible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cover_Type']=data['Cover_Type'].astype('category') # 先將它轉成category的格式\n",
    "data= pd.get_dummies(data, columns=['Cover_Type']) #再將它轉成dummy\n",
    "\n",
    "print('Show the columns after using get_dummies function')\n",
    "print(data.columns)\n",
    "print('Show from Cover_Type_1 to Cover_Type_7  columns')\n",
    "data.iloc[: , 54: ].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing data\n",
    "\n",
    "Now split the dataset into a training set and a test set.\n",
    "\n",
    "We will use the test set in the final evaluation of our model.\n",
    "\n",
    "- num_train: number of training samples -> 80% data\n",
    "- num_test: number of test samples ->20% data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 將資料分成 training 跟 test \n",
    "train = data.sample(frac=0.8,random_state=0)\n",
    "test = data.drop(train.index)\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "\n",
    "print(\"oringinal data shape: \" + str(data.shape)) # =>(581012, 61)\n",
    "print(\"train shape: \" + str(train.shape)) # => (464810, 61)\n",
    "print(\"test shape: \" + str(test.shape)) # =>(116202, 61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split features from labels\n",
    "\n",
    "Separate the target value, or \"label\", from the features. This label is the value that you will train the model to predict.\n",
    "\n",
    "- train_x shape: (464810, 54)\n",
    "- train_y shape: (464810, 7)\n",
    "- test_x shape: (116202, 54)\n",
    "- test_y shape: (116202, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "train_x = \n",
    "train_y = \n",
    "test_x = \n",
    "test_y = \n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"train_x shape: \" + str(train_x.shape)) # => (464810, 54)\n",
    "print(\"train_y shape: \" + str(train_y.shape)) # =>  (464810, 7))\n",
    "print(\"test_x shape: \" + str(test_x.shape)) # =>(116202, 54)\n",
    "print(\"test_y shape: \" + str(test_y.shape)) # =>(116202, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalization (feature scaling)\n",
    "\n",
    "The the sizes and number of bedrooms are at different scales. We perform feature normalization to make learning easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(X):\n",
    "    \"\"\" \n",
    "    X: 訓練資料 \n",
    "    mu: 平均數\n",
    "    sigma: 標準差\n",
    "    \"\"\"  \n",
    "    # hint 若利用np.mean()，需加上`keepdim=True`。\n",
    "    mu = np.mean(X, keepdims = True , axis=0)\n",
    "    sigma = np.std(X, keepdims = True , axis=0)\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "# 利用平均數跟標準差進行標準化\n",
    "def normalize_feat(X, mu, sigma):\n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    normalized_X = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return normalized_X\n",
    "\n",
    "\n",
    "\n",
    "mu, sigma = compute_mean_std(train_x)\n",
    "\n",
    "# 讓訓練資料跟測試資料都進行標準化\n",
    "\n",
    "train_x = normalize_feat(train_x, mu, sigma)\n",
    "test_x = normalize_feat(test_x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "Let's build our model. Here, we'll use a Sequential model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, build_model, since we'll create a second model, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim,learning_rate):\n",
    "    \n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "        layers.Dense(64, activation = YOURCODEHERE, input_shape=(YOURCODEHERE,)),  #input維度為(input_dim,*) output維度為(*,64)\n",
    "\n",
    "        layers.Dense(64, activation=YOURCODEHERE), ## 這裡的activation 可以跟上面的一樣\n",
    "        \n",
    "        keras.layers.Dense(YOURCODEHERE, activation=  YOURCODEHERE) #這裡是最後一層 所以 output維度 跟 activation 會不一樣\n",
    "        ### END CODE HERE ###\n",
    "  ])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    optimizer =   ## 我們使用ADAM當作我們的optimizer,並在裡面設定learing_rate參數 hint : tf.optimizers\n",
    "    ### START CODE HERE ### \n",
    "    model.compile(optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the input dimension and learning rate\n",
    "\n",
    "In order to use our model function, we have to set input dimension and learning rate\n",
    "\n",
    "-  Input dimension : eature number of training set\n",
    "-  Learning rate : learning rate of the gradient descent update rule\n",
    "\n",
    "\n",
    "Here, we use 0.001 as our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "### hint: feature number of training set\n",
    "dim =\n",
    "### END CODE HERE ###\n",
    "learning_rate=0.001\n",
    "\n",
    "model = build_model(input_dim=dim,learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the model\n",
    "\n",
    "Use the `.summary` method to print a simple description of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model for 30 epochs , and record the training and validation accuracy in the `history1` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model.fit(\n",
    " train_x, train_y,\n",
    " epochs= 30, batch_size = 50,\n",
    " ,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the training and validation loss and accuracy plots\n",
    "We use `history1` object to show the training and validation loss plot ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again ,we use `history1` object to show the training and validation accuracy plot ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validaiton'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our model acuuracy\n",
    " Let us use the test data `test_x`and `test_y` to evaluate our model.\n",
    "\n",
    "\n",
    "We can get loss and accuracy from this.\n",
    "\n",
    "hint : we  use `model.evaluate(x,y)` to get loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "loss, accuracy =  \n",
    "### END CODE HERE ###\n",
    "print('test loss:', loss, '\\n Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own model\n",
    "\n",
    "Try different learning rate or optimizer (etc. Stochastic gradient descent(SGD), Momentum, Adam). \n",
    "\n",
    "You can learn more about the usage of optimizer in Keras [here](https://keras.io/zh/optimizers/).\n",
    "\n",
    "Build the model, and plot the loss of train&validation set. Does it perform better? Does it perform as well in test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(input_dim,learning_rate):\n",
    "    \n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(___, activation=___, input_shape=(input_dim,)),  \n",
    "\n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        keras.layers.Dense(7, activation=  'softmax')\n",
    "  ])\n",
    "    optimizer =  \n",
    "    \n",
    "    model.compile(optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the model\n",
    "\n",
    "Use the `.summary` method to print a simple description of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the input dimension and learning rate\n",
    "\n",
    "In order to use our model function, we have to set input dimension and learning rate\n",
    "\n",
    "-  Input dimension : eature number of training set\n",
    "-  Learning rate : learning rate of the gradient descent update rule\n",
    "\n",
    "\n",
    "Here, you can fine-tune  learning rate to   train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "### hint: feature number of training set\n",
    "dim =\n",
    "### END CODE HERE ###\n",
    "learning_rate=\n",
    "\n",
    "model = build_model2(input_dim=dim,learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Train the model , and record the training and validation accuracy in the `history2` object.\n",
    "\n",
    "You can adjust the `epochs` and `batch_size`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(\n",
    " train_x, train_y,\n",
    " epochs= 30, batch_size = 50,\n",
    " ,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the training and validation loss and accuracy plots\n",
    "We use `history2` object to show the training and validation loss plot .## Show the training and validation loss plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again ,we use `history2` object to show the training and validation accuracy plot ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validaiton'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our model acuuracy\n",
    " Let us use the test data `test_x`and `test_y` to evaluate our model.\n",
    "\n",
    "\n",
    "We can get loss and accuracy from this.\n",
    "\n",
    "hint : we  use `model.evaluate(x,y)` to get loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "loss, accuracy =  ## hint :  model.evaluate(x,y)\n",
    "### END CODE HERE ###\n",
    "print('test loss:', loss, '\\n Test accuracy:', accuracy) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
