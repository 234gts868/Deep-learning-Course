{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c4f5211fc0d2e74b0d6aabe9fa8e07164ee07c5"
   },
   "source": [
    "# **Lab 10: Reinforcement Learning with OpenAI Gym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "0f45fbd74eba00eef91dfbb511f73ab06dbc667d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/bolunlin/anaconda3/envs/REIN/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import datetime\n",
    "import keras \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "from time import sleep\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "import warnings\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Gym**\n",
    "\n",
    "Gym is released by Open AI in 2016 (http://gym.openai.com/docs/). Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow.\n",
    "\n",
    "The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.\n",
    "\n",
    "There are many environments that you can learn more. http://gym.openai.com/envs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"enviroment.png\" alt=\"enviroment\" width=\"500\"   align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f62ddd2db99429cd125112bda541723c858f419"
   },
   "source": [
    "# **1. Intro to taxi game environment**\n",
    "\n",
    "The aim of the taxi game is to make sure the taxi can get to the passenger, pick him up and bring him to the drop-off location in the fastest way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d0578768abc4cbf53c4a20d250ef8e39c99497c"
   },
   "source": [
    "\n",
    "\n",
    "**Representations**\n",
    "\n",
    "\n",
    "<table >\n",
    "    <tr><th>Symbol</th><th>Representation</th><th>Example</th></tr>\n",
    "    <tr><td>|</td><td>WALL (Can't pass through)</td><td rowspan = '6' > <img src=\"taxi.png\" alt=\"taxi\"   align=\"center\" />  <td></tr>\n",
    "    <tr><td>Yellow bar</td><td>Taxi Current Location </td></tr>\n",
    "    <tr><td>Letters</td><td>Locations </td></tr>\n",
    "    <tr><td>Blue letter</td><td> Pick up Location  </td></tr>\n",
    "      <tr><td>urple letter</td><td> Drop-off Location   </td></tr>\n",
    "        <tr><td>Green letter </td><td> Taxi turn green once passenger board   </td></tr>\n",
    "   </table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b321b04f669571df210d6676265b9273d3017a9d"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3') # load taxi enviroment\n",
    "env.reset()# initalize the vironment and this will return a integer\n",
    "env.render() ## visualize the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that **env.reset() resetting the environment** will return an integer. This number will be our initial state. All possible states in this environment are represented by an integer ranging from 0 to 499. We can determine the total number of possible states using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0004dc2b02965be1f47ec64998578b91a1e819f"
   },
   "outputs": [],
   "source": [
    "env.observation_space.n   #Total no. of states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ddf91afb34e02d9a3ca72442516b2c9ac09482b3"
   },
   "source": [
    "**Actions (6 in total)**\n",
    "\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3dfff01881abd8bc8c4fd6ade6199e0be8fd0fa"
   },
   "outputs": [],
   "source": [
    "env.action_space.n   #Total no. of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4a0664e73e126d75e1139ed1a1f6e303704088a"
   },
   "outputs": [],
   "source": [
    "env.env.s = 122\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d23b96ffb38b239dcc551d81ae3a6c211c3db73"
   },
   "outputs": [],
   "source": [
    "env.step(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "201acf86a6308d8220cb1f474af31c8f64450ab9"
   },
   "source": [
    "Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "\n",
    "The 4 elements returned are:\n",
    "\n",
    "-  **Observation (object)**: the state the environment is in or an environment-specific object representing your observation of the environment.\n",
    "-  **Reward (float)**: Reward achieved by the previous action. \n",
    "    -  +20: Last step when we successfully pick up a passenger and drop them off at their desired location\n",
    "    -  -1: for each step in order for the agent to try and find the quickest solution possible\n",
    "    -  -10: every time you incorrectly pick up or drop off a passenger\n",
    "-  **Done (boolean)**: whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, you lost your last life.)\n",
    "-  **Info (dict)**: Can be ignored, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ed8fa4353fb0bb7c2f831099d6d228c421829bf"
   },
   "outputs": [],
   "source": [
    "env.render()  #view state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dca645ec054b91775aecd06acdde55c3450ce7e"
   },
   "source": [
    "The function (env.P) below can be used to see the relevant states and rewards for each action taken in that particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a45d784b8e8a22c54bed3a3f6f918021a721e1c"
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.env.P[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd7f8693f05c30915cba7f0a712ea24b82035363"
   },
   "source": [
    "# **2. Random Search**\n",
    "Let's start with the simplest way to train our agent to complete this task. The agent would just take random steps at every state until he completes the task (picking the passenger and dropping him off at the drop-off location). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create the environment\n",
    "We use the Taxi-v3 as our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define random search method\n",
    "\n",
    "- This function will return counts that the method how many steps to complete this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd8addbf5b06d604052757c34c35f803c280684a"
   },
   "outputs": [],
   "source": [
    "def random_policy_steps_count():\n",
    "    state = env.reset()\n",
    "    counter = 0\n",
    "    reward = None\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(env.action_space.sample())  \n",
    "        counter += 1\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  Let's use Random Search to complete the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b64e536320cff64dc87c7c099e66bdecf766c18"
   },
   "outputs": [],
   "source": [
    "counts = [random_policy_steps_count() for i in range(1000)]\n",
    "sns.distplot(counts)\n",
    "plt.title(\"Distribution of number of steps needed\")\n",
    "print(\"An agent using Random search takes about an average of\"\" \" + str(int(np.mean(counts)))\n",
    "      + \" \"\"steps to successfully complete its mission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, it is not the most efficient way to complete this. Let's try to use Q-learning with epsilon-greedy method to more effectively let our agent complete this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "279fdaba96989912e3edc7d3473a88ddef6500ef"
   },
   "source": [
    "# **3.Q-learning with epsilon-greedy method** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create the environment\n",
    "We use the Taxi-v3 as our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The Q-learning algorithm\n",
    "\n",
    "The algorithm for Q-learning is as follows:\n",
    "\n",
    "- Step 1 : Initialize the Q-table by all zeros.\n",
    "- Step 2 : Create the hyperparameters  learning rate, gamma, episodes, epsilon, decay_rate, max_epsilon, min_epsilon\n",
    "- Step 3: Start exploring actions: For each state, select any one among all possible actions for the current state (S) and travel to the next state (S') as a result of that action (a).\n",
    "- Step 4  : Update Q-table value :For all possible actions from the state (S') select the one with the highest Q-value with probability (1-epsilon) and select a random action with probability (epsilon).   \n",
    "This is to balance the exploration and exploitation actions. using the equation:  **Q(state,action) <- (1−α) * Q(state,action) + α(reward + γ * max<sub>a</sub>Q(next state,all actions))**\n",
    "- Step 5 : Set the next state as the current state. If goal state is reached, then end and repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Initialize the Q-table\n",
    "We initialize the Q-table by all zeros.  \n",
    "Q-table shape will be **State size ×Action size**  \n",
    "hint : **[np.zeros()](https://numpy.org/devdocs/reference/generated/numpy.zeros.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Action size \", action_size)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "print(\"State size \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table =  ## CODE HERE##\n",
    "print(\"Q table shape is \",Q_table.shape)\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs  Should be  \n",
    "Q table shape is  (500, 6)  \n",
    "[[0. 0. 0. 0. 0. 0.]  \n",
    " [0. 0. 0. 0. 0. 0.]  \n",
    " [0. 0. 0. 0. 0. 0.]  \n",
    " ...  \n",
    " [0. 0. 0. 0. 0. 0.]  \n",
    " [0. 0. 0. 0. 0. 0.]  \n",
    " [0. 0. 0. 0. 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Create the hyperparameters\n",
    "This cell will  set up the following Q learning hyperparameters.\n",
    "1. **alpha** :  Learning Rate --> Extent to which our Q-values are being updated in every iteration.\n",
    "2. **gamma** :  Discount Rate --> How much importance we want to give to future rewards\n",
    "3. **epsilon** : Exploration Rate --> Probability of selecting random action instead of the 'optimal' action\n",
    "4. **episodes**: Total episodes to train on  \n",
    "&emsp; **max_epsilon**  : Exploration probability at start  \n",
    "&emsp; **min_epsilon** : Minimum exploration probability   \n",
    "&emsp; **decay_rate** :Exponential decay rate for exploration prob  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7           # Learning rate\n",
    "gamma = 0.618                 # Discounting rate\n",
    "\n",
    "episodes = 50000        # Total episodes\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Action function\n",
    "We choose a random number as action   \n",
    "**hint** use [random.uniform()](https://docs.python.org/3/library/random.html) function to create number between 0 and  1\n",
    "\n",
    "\n",
    "**if the number greater than epsilon**  \n",
    "&emsp;  take the biggest Q value for this state.\n",
    "\n",
    "**else**  \n",
    "&emsp;  use a random action as our choice \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(Q_table , epsilon,state):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        Q_table : current Q-table\n",
    "        epsilon:  probability of selecting random action instead of the 'optimal' action\n",
    "        state: current state\n",
    "    Returns:\n",
    "        action :  action based on the current state \n",
    "\n",
    "    \"\"\"    \n",
    "    random_number_tradeoff = ## CODE HERE##\n",
    "    if random_number_tradeoff > epsilon:\n",
    "        action =  ## CODE HERE##    # Exploit learned values by choosing max values   \n",
    "    else:\n",
    "        action = env.action_space.sample() # Explore action space randomly     \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Update function\n",
    "\n",
    "Update Q-table values using the equation:  **Q(state,action) <- (1−α) * Q(state,action) + α(reward + γ * max<sub>a</sub>Q(next state,all actions))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def update(Q_table,state,action,next_state,reward,alpha,gamma):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        Q_table : current Q-table\n",
    "        state : current state\n",
    "        action : current action\n",
    "        next_state : next state\n",
    "        reward : reward in the current action\n",
    "        alpha : learning Rate --> Extent to which our Q-values are being updated in every iteration.\n",
    "        gamma : discount Rate --> How much importance we want to give to future rewards\n",
    "    Returns:\n",
    "        updated Q-learning  table \n",
    "\n",
    "    \"\"\"    \n",
    "    old_value = Q_table[state, action]\n",
    "    next_max = np.max(Q_table[next_state])\n",
    "\n",
    "    Q_table[state, action]  =  ## CODE HERE##\n",
    "    return  Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Q learning train function\n",
    "Merge action and update function to trains Q table\n",
    "\n",
    "loop episodes times:  \n",
    "&emsp; reset our environment  \n",
    "&emsp; while not done:  \n",
    "&emsp; &emsp; choose an optimal **action**   \n",
    "&emsp; &emsp; execution action  \n",
    "&emsp; &emsp; **update** our Q-table  \n",
    "&emsp; &emsp; use the state for next action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eac8b1cafdbc5dd6a6f83f37e4b6f1e9981e7332"
   },
   "outputs": [],
   "source": [
    "def Q_learning_train(env,alpha,Q_table,gamma,epsilon,episodes): \n",
    "    \"\"\"Q Learning Algorithm with epsilon greedy \n",
    "\n",
    "    Args:\n",
    "        env: Environment \n",
    "        Q_table : initialized q-table \n",
    "        alpha: Learning Rate --> Extent to which our Q-values are being updated in every iteration.\n",
    "        gamma: Discount Rate --> How much importance we want to give to future rewards\n",
    "        epsilon: Probability of selecting random action instead of the 'optimal' action\n",
    "        episodes: number of episodes to train on\n",
    "\n",
    "    Returns:\n",
    "        Q-learning Trained table\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for episode in range(1, episodes+1):\n",
    "        \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "           \n",
    "            action=   #### CODE　HERE#### hint : call act() finction\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action) \n",
    "            \n",
    "            Q_table = #### CODE　HERE####  ## hint : call update() finction\n",
    "\n",
    "            state = next_state\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "        if episode % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {episode}\")\n",
    "    print(\"Training finished.\\n\")\n",
    "    return Q_table    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 Let's train our Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d1731a715762ac303703946d2d67c1a31e7230b"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "Q_table = Q_learning_train(env,alpha,Q_table,gamma,epsilon,episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.7 Create our Q learning policy\n",
    " we will our trained Q-table create a policy matrix that each states should have a best action.  \n",
    " hint : we can use [np.argmax()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) to find best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial our policy matrix\n",
    "policy = np.zeros([env.env.nS, env.env.nA]) / env.env.nA\n",
    "\n",
    "for state in range(env.env.nS):  #for each states\n",
    "    best_act =  ### CODE HERE ### #find best action in this state\n",
    "    policy[state] = np.eye(env.env.nA)[best_act]  #update \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.8 Count function\n",
    "Count how many average  steps  the policy will use in each taxi game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(policy):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, reward, done, info = env.step(np.argmax(policy[curr_state]))  \n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68e98ba01e503d5df27d93f502e03cb82f79f2dc"
   },
   "outputs": [],
   "source": [
    "Q_Learning_counts = count(policy)\n",
    "Q_counts = [count(policy) for i in range(1000)]\n",
    "print(\"An agent using a policy which has been improved using Q-learning takes about an average of \" + str(int(np.mean(Q_counts)))\n",
    "      + \" steps to successfully complete its mission.\")\n",
    "sns.distplot(Q_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.9 Use our Q-table to play Taxi ! \n",
    "After 50 000 episodes, our Q-table can be used as a \"cheatsheet\" to play Taxi.  \n",
    "By running this cell you can see our agent playing Taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "counts =[]\n",
    "total_test_episodes = 100   ## We try 100 episodes in our test \n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "        \n",
    "        action = np.argmax(Q_table[state])  # Take the action (index) that have the maximum expected future reward given that state \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        total_rewards += reward\n",
    "        count +=1\n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            counts.append(count)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Use Q-learning takes about an average \"+str(sum(rewards)/total_test_episodes) +\" in the taxi game.\" )\n",
    "print (\"Use Q-learning takes about an average of \"+str(sum(counts)/total_test_episodes) +\" to successfully finish the taxi game.\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cda548224d3bc9bae9dcc4feeeba27150467eae1"
   },
   "source": [
    "# **4. Let's watch how our Q-learning works in action**\n",
    "\n",
    "We can also view it animated by running the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52b4079c117a61c15bb53da6d099542e6f2d8a39"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def view_policy_anim(policy):\n",
    "    penalties, reward = 0, 0\n",
    "\n",
    "    frames = [] # for animation\n",
    "\n",
    "    done = False\n",
    "    curr_state = env.reset()\n",
    "    while not done:\n",
    "        action = np.argmax(policy[curr_state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        curr_state = state\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "    def print_frames(frames):\n",
    "        for i, frame in enumerate(frames):\n",
    "            clear_output(wait=True)\n",
    "            print(frame['frame'])\n",
    "            print(f\"Timestep: {i + 1}\")\n",
    "            print(f\"State: {frame['state']}\")\n",
    "            print(f\"Action: {frame['action']}\")\n",
    "            print(f\"Reward: {frame['reward']}\")\n",
    "            sleep(.2)\n",
    "\n",
    "    print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c728056c17ffb40e59a936cce999139f4b77e13e"
   },
   "outputs": [],
   "source": [
    "view_policy_anim(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
