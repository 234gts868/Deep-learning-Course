{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 : Logistic Regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "在這個lab會實作簡單的logistic regression\n",
    "回歸，將它應用在分類的模型，利用training data 訓練完成後，再利用testing data來檢驗模型的好壞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此提供原生資料train.csv。我們的任務就是利用資料裡的欄位當成特徵值，搭配 logistic regression的計算流程去預測income，income大於50K為1，小於等於50K為0，最後利用測試資料來測準確率。<br>已經完成feature format的檔案為X_train.csv、Y_train.csv、X_test.csv、Y_test.csv。\n",
    "\n",
    "<br>訓練資料是用來訓練模型的，而測試資料則是來評估訓練後模型的準確率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.csv : 原生訓練資料<br>X_train.csv: 處理後的訓練資料X，有32561筆資料，每筆資料的特徵維度(數)是106。<br>Y_train.csv: 處理後的訓練資料Ｙ，有32561筆資料，每筆資料有一個標籤。<br>X_test.csv: 處理後的測試資料Ｘ，有16281筆資料，每筆資料的特徵維度(數)是106。<br>Y_test.csv: 處理後的測試資料Ｙ，有16281筆資料，每筆資料代表一個標籤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料欄位\n",
    "- income: >50K, <=50K\n",
    "- age: continuous.\n",
    "- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "- fnlwgt: continuous.\n",
    "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "- education-num: continuous.\n",
    "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "- sex: Female, Male.\n",
    "- capital-gain: continuous.\n",
    "- capital-loss: continuous.\n",
    "- hours-per-week: continuous.\n",
    "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先匯入模組去檢驗資料，我們會利用這些模組的函式幫忙取資料、做運算和畫圖等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "這邊會得到各兩個訓練資料跟測試資料，每個列代表一筆資料，所以資料的總數 = 列的數，利用資料的特徵去預測income。\n",
    "\n",
    "- train_x的維度是(32561, 106)，32561為資料數，106為特徵數。\n",
    "- train_y的維度是(32561, 1)，32561為資料數，1為標籤數。\n",
    "- test_x的維度是(16281, 106)，16281為資料數，106為特徵數。\n",
    "- test_y的維度是(16281, 1)，16281為資料數，1為標籤數。\n",
    "\n",
    "特徵 :資料的描述，如資料欄位中的age、workclass...。<br>\n",
    "標籤 :為每筆資料要預測的目標或者正確答案，資料有多少筆就代表有多少標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>Federal-gov</th>\n",
       "      <th>Local-gov</th>\n",
       "      <th>Never-worked</th>\n",
       "      <th>Private</th>\n",
       "      <th>...</th>\n",
       "      <th>Puerto-Rico</th>\n",
       "      <th>Scotland</th>\n",
       "      <th>South</th>\n",
       "      <th>Taiwan</th>\n",
       "      <th>Thailand</th>\n",
       "      <th>Trinadad&amp;Tobago</th>\n",
       "      <th>United-States</th>\n",
       "      <th>Vietnam</th>\n",
       "      <th>Yugoslavia</th>\n",
       "      <th>?_native_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  sex  capital_gain  capital_loss  hours_per_week   Federal-gov  \\\n",
       "0   39   77516    1          2174             0              40             0   \n",
       "1   50   83311    1             0             0              13             0   \n",
       "2   38  215646    1             0             0              40             0   \n",
       "3   53  234721    1             0             0              40             0   \n",
       "4   28  338409    0             0             0              40             0   \n",
       "\n",
       "    Local-gov   Never-worked   Private        ...          Puerto-Rico  \\\n",
       "0           0              0         0        ...                    0   \n",
       "1           0              0         0        ...                    0   \n",
       "2           0              0         1        ...                    0   \n",
       "3           0              0         1        ...                    0   \n",
       "4           0              0         1        ...                    0   \n",
       "\n",
       "    Scotland   South   Taiwan   Thailand   Trinadad&Tobago   United-States  \\\n",
       "0          0       0        0          0                 0               1   \n",
       "1          0       0        0          0                 0               1   \n",
       "2          0       0        0          0                 0               1   \n",
       "3          0       0        0          0                 0               1   \n",
       "4          0       0        0          0                 0               0   \n",
       "\n",
       "    Vietnam   Yugoslavia  ?_native_country  \n",
       "0         0            0                 0  \n",
       "1         0            0                 0  \n",
       "2         0            0                 0  \n",
       "3         0            0                 0  \n",
       "4         0            0                 0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定 data_path，依據放資料的資料夾去設。\n",
    "dir_data = './data/'\n",
    "\n",
    "# 讀取檔案\n",
    "data = pd.read_csv(dir_data+'X_train.csv')\n",
    "train_x = np.array(pd.read_csv(dir_data+'X_train.csv'))\n",
    "train_y = np.array(pd.read_csv(dir_data+'Y_train.csv'))\n",
    "test_x = np.array(pd.read_csv(dir_data+'X_test.csv'))\n",
    "test_y = np.array(pd.read_csv(dir_data+'Y_test.csv'))\n",
    "# 檢視我們的資料，這邊先呈現5筆，他會有106個欄位，也就是特徵值。\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know how many training and test samples\n",
    "\n",
    "許多在深度學習的錯誤來自於矩陣或向量的維度不相符。如果能了解資料的維度，可降低錯誤的發生。<br>\n",
    "寫出下方的程式碼，可得知資料維度。\n",
    "\n",
    "```\n",
    "num_train : 訓練資料的數量\n",
    "num_test : 測試資料的數量\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料的數目: num_train = 32561\n",
      "測試資料的數量: num_test = 16281\n",
      "train_x的維度: (32561, 106)\n",
      "train_y的維度: (32561, 1)\n",
      "test_x的維度: (16281, 106)\n"
     ]
    }
   ],
   "source": [
    "# 取得長度可知道有幾筆。\n",
    "\n",
    "# Your code here\n",
    "num_train = len(train_x)\n",
    "num_test = len(test_x)\n",
    "# End your code\n",
    "\n",
    "# 看訓練資料跟測試資料的維度 (資料數 , 特徵數)。\n",
    "print(\"訓練資料的數目: num_train = \" + str(num_train)) # 32561\n",
    "print(\"測試資料的數量: num_test = \" + str(num_test)) # 16281\n",
    "print(\"train_x的維度: \" + str(train_x.shape)) # (32561, 106)\n",
    "print(\"train_y的維度: \" + str(train_y.shape))# (32561, 1)\n",
    "print(\"test_x的維度: \" + str(test_x.shape)) # (16281, 106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dimensions\n",
    "改變訓練跟測試資料的維度變成 (特徵數，資料數)，讓資料能夠做矩陣運算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "改變後的維度\n",
      "train_x : (106, 32561)\n",
      "train_y : (1, 32561)\n",
      "test_x  : (106, 16281)\n",
      "test_y  : (1, 16281)\n"
     ]
    }
   ],
   "source": [
    "# 這邊使用轉置函數T去讓去讓維度做交換。\n",
    "# Your code here \n",
    "train_x = train_x.T\n",
    "test_x = test_x.T\n",
    "train_y = train_y.T\n",
    "test_y = test_y.T\n",
    "# End your code\n",
    "\n",
    "print(\"\\n改變後的維度\")\n",
    "print(\"train_x : \" + str(train_x.shape))# (106, 32561)\n",
    "print(\"train_y : \" + str(train_y.shape))# (1, 32561)\n",
    "print(\"test_x  : \" + str(test_x.shape))# (106, 16281)\n",
    "print(\"test_y  : \" + str(test_y.shape))# (1, 16281)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalization (feature scaling)\n",
    "\n",
    "特徵間的範圍會不同，所以利用特徵標準化去讓範圍一致以便更快收斂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前五個特徵的平均值 : \n",
      " [[3.85816468e+01]\n",
      " [1.89778367e+05]\n",
      " [6.69205491e-01]\n",
      " [1.07764884e+03]\n",
      " [8.73038297e+01]]\n",
      "前五個特徵的平均值和標準差 : \n",
      " [[1.36402231e+01]\n",
      " [1.05548357e+05]\n",
      " [4.70499205e-01]\n",
      " [7.38517868e+03]\n",
      " [4.02954031e+02]]\n"
     ]
    }
   ],
   "source": [
    "# 利用平均數跟標準差的公式算出mu跟sigma。\n",
    "def compute_mean_std(X):\n",
    "    \"\"\" \n",
    "    X: 訓練資料 \n",
    "    mu: 平均數\n",
    "    sigma: 標準差\n",
    "    \"\"\"  \n",
    "    # Your code here\n",
    "    # hint 若利用np.mean()，需加上`keepdim=True`。\n",
    "    mu = np.mean(X, keepdims = True , axis=1)\n",
    "    sigma = np.std(X, keepdims = True , axis=1)\n",
    "    # End your code\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "# 利用平均數跟標準差進行標準化。\n",
    "def normalize_feat(X, mu, sigma):\n",
    "    \n",
    "    # Your code here\n",
    "    # 寫出標準化的公式。\n",
    "    normalized_X = (X - mu) / sigma\n",
    "    # End your code\n",
    "    return normalized_X\n",
    "\n",
    "\n",
    "# Your code here\n",
    "mu, sigma = compute_mean_std(train_x)\n",
    "# End your code\n",
    "\n",
    "print('前五個特徵的平均值 : \\n', mu[:5])# [[3.85816468e+01] [1.89778367e+05][6.69205491e-01][1.07764884e+03][8.73038297e+01]]\n",
    "print('前五個特徵的平均值和標準差 : \\n', sigma[:5])#  [[1.36402231e+01][1.05548357e+05][4.70499205e-01][7.38517868e+03][4.02954031e+02]]\n",
    "\n",
    "# 讓訓練資料跟測試資料都進行標準化。\n",
    "# 測試資料的標準化的參數需跟訓練資料一樣，不然會影響預測結果。\n",
    "train_x = normalize_feat(train_x, mu, sigma)\n",
    "test_x = normalize_feat(test_x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing parameters \n",
    "在下方的cell進行參數初始化。我們這個lab的參數會是w跟b，w會是一個矩陣，可以利用np.zeros()來實作，而b是一個純量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \n",
    "    \"\"\"\n",
    "    這個函式會建立一個(dim, 1)維度的w，然後b會初始化為0。\n",
    "    dim -- w的維度大小，也是特徵的數量。\n",
    "    w -- (dim, 1)\n",
    "    b -- 純量\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # hint : w可用np.zeros()做初始化\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    # End your code\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# 初始化參數。\n",
    "dim = train_x.shape[0]\n",
    "w, b = initialize_with_zeros(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Get the cost and gradients\n",
    "利用損失函數跟梯度下降來幫助訓練參數。\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "我們會利用propagate function:\n",
    "\n",
    "我們利用正向傳播 (Forward propagation)去算資料X的損失:\n",
    "- 會有資料X，X為二維矩陣\n",
    "- 去計算 : $Z = w^T X + b\\quad(1)$\n",
    "<br><br>\n",
    "- 然後利用sigmoid函數去算預測值。\n",
    "$$A = \\sigma(Z) = \\frac{1}{1 + e^{-Z}}\\quad(2)$$    \n",
    "<br><br>\n",
    "- 利用損失函數去算預測值與實際值的落差。 $$L =-\\frac{1}{N} [\\sum_{n=1}^N y^nlog(a^n) + (1-y^n)log(1-a^n)]\\quad(3)$$\n",
    "\n",
    "更新參數的部分，可以使用以下的式子得到梯度: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{N}X(A-Y)^T\\quad(4)$$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (a^{(i)}-y^{(i)})\\quad(5)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求損失\n",
    "def compute_loss(A , Y) :\n",
    "    '''\n",
    "    A -- 預測值 (1, 訓練資料數)\n",
    "    Y -- 實際值 (1, 訓練資料數)\n",
    "    '''\n",
    "    # 參考公式 (3)。\n",
    "    # Your code here\n",
    "    loss = -(Y * np.log(A) + (1 - Y) * np.log(1 - A)).mean()\n",
    "    # end your code\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 帶入公式 (2)，範圍會被限制在0~1之間。\n",
    "def sigmoid(z) :\n",
    "    # Your code here\n",
    "    s = 1 / (1.0 + np.exp(-z))\n",
    "    # End your code\n",
    "    return s\n",
    "\n",
    "# 算出參數 (w, b)的梯度。\n",
    "def propagate(w, b, X, Y):\n",
    "    \n",
    "    \"\"\"\n",
    "    w -- (特徵數, 1)\n",
    "    b -- 一個純量\n",
    "    X -- data of size (特徵數, 資料數)\n",
    "    Y -- 實際值 (1, 資料數)\n",
    "    \n",
    "    loss -- 預測值與實際值的落差。\n",
    "    dw -- w的梯度，維度跟w一樣。\n",
    "    db -- b的低度，維度跟b一樣。\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    N = X.shape[1]\n",
    "    \n",
    "    # 正向傳播，利用公式 (1)和公式 (2)求得預測值A，再利用A與Y (實際值)來求損失。\n",
    "    \n",
    "    # Your code here\n",
    "    # 利用公式 (1)算預測值。\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    \n",
    "    # 利用公式 (2)讓預測值範圍限制在0到1之間。\n",
    "    A = sigmoid(Z)\n",
    "    # End your code\n",
    "    \n",
    "    # 限制A的範圍。\n",
    "    A = np.clip(A, 1e-6, 1-1e-6)\n",
    "    \n",
    "    # Your code here\n",
    "    # 呼叫 compute_loss()，利用預測值與實際值去算損失\n",
    "    loss = compute_loss(A, Y)\n",
    "    # End your code\n",
    "    \n",
    "    # 反向傳播，這邊會算出參數的梯度，我們之後利用此梯度來更新參數並讓損失函數最小化。\n",
    "    # Your code here\n",
    "    dw = np.dot(X, (A-Y).T) / N\n",
    "    db = np.sum(A-Y) / N\n",
    "\n",
    "    # End your code\n",
    "\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    loss = np.squeeze(loss)\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    # 把參數的梯度存起來，之後會用來做更新。\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "利用公式(1)和公式(2)算出預測值，因為我們是要做0/1的分類，所以會再對每一筆預測值依是否大於0.5轉成0或1，這樣就可以跟實際值做比較。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算預測值\n",
    "def predict(w, b, X):\n",
    "\n",
    "    '''\n",
    "    w -- (特徵數, 1)\n",
    "    b -- 純量\n",
    "    X -- (特徵數, 資料數)\n",
    "\n",
    "    Y_prediction -- 為一個numpy矩陣，包含對X的預測值(0/1)。\n",
    "    '''\n",
    "    # 設一陣列存預測值\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "\n",
    "    # Your code here\n",
    "    # 利用公式 (1)算預測值。\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    # 利用公式 (2)讓預測值範圍限制在0到1之間。\n",
    "    A = sigmoid(Z)\n",
    "    # End your code\n",
    "    \n",
    "    # 預測值若大於0.5為1，反之為0，可用 np.where()來分類\n",
    "    Y_prediction[0, :m] = np.where(A[0, :m] > 0.5, 1, 0)\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "- 計算損失跟梯度。\n",
    "- 利用參數的梯度來更新參數。\n",
    "\n",
    "目的是最小化損失函數 $L$來學習$w$ 和 $b$. 以參數 $\\theta$來說, 更新規則為$ \\theta = \\theta - \\eta \\text{ } d\\theta$,\n",
    "其中$\\eta$ 為學習率，用來調整更新的幅度。\n",
    "\n",
    "我們會紀錄每次更新參數後對損失的影響。<br>\n",
    "另外，也會呼叫predict()得到每筆資料預測值，算出每次更新參數後的準確率，並記錄起來。\n",
    "我們記錄的損失跟準確率會以學習曲線的方式呈現，可看出藉由參數的更新是否會讓模型的預測效果更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新參數\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_loss = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    此函數會藉由梯度下降函式來更新w跟b。\n",
    "    \n",
    "    w -- (特徵數, 1)\n",
    "    b -- 一個純量\n",
    "    X -- data of size (特徵數, 資料數)\n",
    "    Y -- 實際值 (1,資料數)\n",
    "    num_iterations -- 要更新的次數。\n",
    "    learning_rate -- 梯度下降的學習率。\n",
    "    print_loss -- 當True，則印出損失。\n",
    "    Y_prediction -- 為一個numpy矩陣，包含對X的預測值(0/1)。\n",
    "    \n",
    "    Returns:\n",
    "    params -- 為一個dictionary (資料結構)，裡面包含w, b。\n",
    "    grads -- 為一個dictionary，裡面包含dw, db。\n",
    "    losses -- 為一個list，包含所有更新參數時紀錄的損失，會用來畫學習曲線。\n",
    "    \n",
    "    Tips:\n",
    "        1) 利用propagate()去計算目前參數的損失跟梯度。\n",
    "        2) 利用梯度下降的公式去更新w跟b。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 用來存每次損失的值，目的是看出損失的變化。\n",
    "    losses = []\n",
    "    # 紀錄每筆資料的accuracy。\n",
    "    accuracy_temp = 0\n",
    "    # 用來存每次accuracy的值，目的是看出accuracy的變化。\n",
    "    accuracy = []\n",
    " \n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # 呼叫propagate()計算梯度跟損失函數的值。\n",
    "        # Your code here\n",
    "        grads, loss = propagate(w, b, X, Y)\n",
    "        # End your code\n",
    "        \n",
    "        # 得到各自的梯度。\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # 更新參數。\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # 紀錄每10次epoch的損失跟準確率，epoch就是訓練的次數。\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            \n",
    "            # 會把每次的損失加進losses。\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Your code here\n",
    "            # 呼叫predict()，算出資料X的預測值。\n",
    "            Y_prediction = predict(w, b, X)\n",
    "            # End your code\n",
    "            \n",
    "             # 算準確率 : 利用求得的Y_prediction跟我們的實際值去做比較，這邊是算兩者有相同數量，再除全部的資料數。\n",
    "            accuracy_temp = np.mean(Y_prediction == Y)\n",
    "            \n",
    "            accuracy.append(accuracy_temp)\n",
    "            \n",
    "        # 印出每100次epoch的損失狀況。\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, loss))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, losses , accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all functions into a model\n",
    "\n",
    "我們將前面的函式整合起來，跑一次logistic regression的流程。<br>\n",
    "初始化 -> 算出梯度跟損失 -> 更新參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_x, train_y, test_x, test_y, num_iterations, learning_rate, print_loss=False):\n",
    "    \"\"\"\n",
    "    這邊建立一個logistic regression的模型來呼叫前面的函式。\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- (特徵數, 訓練資料數)\n",
    "    Y_train -- (1, 訓練資料數)\n",
    "    X_test --  (特徵數, 測試資料數)\n",
    "    Y_test --  (1, 測試資料數)\n",
    "    num_iterations -- 要更新的次數。\n",
    "    learning_rate -- 梯度下降的學習率。\n",
    "    print_loss -- 當True，則印出損失。\n",
    "    Y_prediction -- 為一個numpy矩陣，包含對X的預測值(0/1)。\n",
    "    \n",
    "    Returns:\n",
    "    d -- 為一個dictionary，包含model的資訊。\n",
    "    \"\"\"\n",
    "    # 使用initialize_with_zeros()來初始化參數，w跟b。\n",
    "    w, b = initialize_with_zeros(train_x.shape[0])\n",
    "    \n",
    "    \n",
    "    # 利用optimize()算出損失跟準確率，且算出梯度去更新參數。\n",
    "    # Your code here\n",
    "    params,grads,losses, accuracy = optimize(w,b,train_x,train_y,num_iterations,learning_rate,print_loss)\n",
    "    # end your code\n",
    "    \n",
    "    # 從params中取得w跟b。\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    # \"訓練完\"模型後的預測值。\n",
    "    Y_prediction_train = predict(w,b,train_x)\n",
    "    Y_prediction_test = predict(w,b,test_x)\n",
    "    \n",
    "    # 印出訓練的損失。\n",
    "    print(\"train loss: {}\".format(losses[-1]))\n",
    "\n",
    "    # 將所有的變數全存起來。\n",
    "    d = {\"loss\": losses,\n",
    "         \"accuarcy\" : accuracy,\n",
    "         \"Y_prediction_test\": Y_prediction_test,\n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.323456\n",
      "Cost after iteration 200: 0.318884\n",
      "Cost after iteration 300: 0.317393\n",
      "Cost after iteration 400: 0.316733\n",
      "Cost after iteration 500: 0.316395\n",
      "Cost after iteration 600: 0.316205\n",
      "Cost after iteration 700: 0.316090\n",
      "train loss: 0.3160215353350179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCost after iteration 0: 0.693147\\nCost after iteration 100: 0.323456\\nCost after iteration 200: 0.318884\\nCost after iteration 300: 0.317393\\nCost after iteration 400: 0.316733\\nCost after iteration 500: 0.316395\\nCost after iteration 600: 0.316205\\nCost after iteration 700: 0.316090\\ntrain loss: 0.3160215353350179\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = model(train_x, train_y, test_x, test_y, num_iterations=800, learning_rate=0.5, print_loss=True)\n",
    "'''\n",
    "Cost after iteration 0: 0.693147\n",
    "Cost after iteration 100: 0.323456\n",
    "Cost after iteration 200: 0.318884\n",
    "Cost after iteration 300: 0.317393\n",
    "Cost after iteration 400: 0.316733\n",
    "Cost after iteration 500: 0.316395\n",
    "Cost after iteration 600: 0.316205\n",
    "Cost after iteration 700: 0.316090\n",
    "train loss: 0.3160215353350179\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XucXHV9//HXe2ZnNhdyNcstCSTaACIK1IBavOAFjK0F+9BqaKvoz5bSn6nXWqFaVPzRh9WqtRUvaIO2FVGx1UhTU7TiBUWz0YAkGAgBzBqBhYQQyGVvn98f5zubk9mZ3c3lZCbJ+/l4zGPmfM/3e85ndnbns+f7Ped7FBGYmZmNptTqAMzMrP05WZiZ2ZicLMzMbExOFmZmNiYnCzMzG5OThZmZjcnJwixH0n9LurjVcZi1GycLawuS7pP0klbHEREvi4gvtDoOAEk3S/rTg7CfTklLJT0m6QFJbx+l7uslDUp6PPc4t+gYrfU6Wh2A2cEiqSMiBlodB7RXLMD7gAXAicCxwHclrY2IbzWp/+OIeO7BCs7ag48srO1Jermk1ZIelfQjSc/IrbtM0j2StklaK+kPcuteL+kWSR+TtBl4Xyr7oaR/kLRF0r2SXpZrM/zf/Djqzpf0/bTvb0u6WtK/N3kP50rqkfQuSQ8A10qaIelGSb1p+zdKmpPqXwU8D/hE+u/9E6n8FEk3SdosaZ2kVx+AH/HrgA9ExJaIuBP4LPD6A7BdO4w4WVhbk/TbwFLgz4EnAZ8BlknqTFXuIftSnQa8H/h3ScflNvEsYANwNHBVrmwdMAv4EPAvktQkhNHqXgf8NMX1PuC1Y7ydY4GZZP/BX0L293dtWj4B2AF8AiAi3g38AFgSEUdFxBJJk4Gb0n6PBi4CPinpaY12JumTKcE2etye6swAjgduyzW9DWi4zeRMSQ9LukvS30pyD8URwMnC2t2fAZ+JiJ9ExGAaT9gFPBsgIr4aEZsiYigivgzcDZyda78pIv45IgYiYkcquz8iPhsRg8AXgOOAY5rsv2FdSScAZwFXRERfRPwQWDbGexkC3hsRuyJiR0Q8EhFfi4jtEbGNLJm9YJT2Lwfui4hr0/v5GfA14FWNKkfE/42I6U0etaOzo9Lz1lzTrcCUJjF8HziNLFm9kixhvXOM922HAScLa3cnAu/I/1cMzCX7bxhJr8t1UT1K9kU2K9d+Y4NtPlB7ERHb08ujGtQbre7xwOZcWbN95fVGxM7agqRJkj4j6X5Jj5F9EU+XVG7S/kTgWXU/iz8mO2LZV4+n56m5sqnAtkaVI2JDRNybkvMvgCtpkqzs8OJkYe1uI3BV3X/FkyLiS5JOJOtfXwI8KSKmA3cA+S6loqZV/g0wU9KkXNncMdrUx/IO4GTgWRExFXh+KleT+huB79X9LI6KiL9otDNJn647ayn/WAMQEVvSezk91/R0YM0Y7yX/npp14dlhxMnC2klF0oTco4MsGVwq6VnKTJb0e5KmAJPJvqx6ASS9gezIonARcT/QTTZoXpX0HOD393IzU8jGKR6VNBN4b936B4En55ZvBE6S9FpJlfQ4S9JTm8R4aUomjR75MYl/Bd6TBtxPIev6+3yjbUp6maRj0utTgL8FvrGX79sOQU4W1k6Wk3151h7vi4husi+vTwBbgPWkM3UiYi3wEeDHZF+sTwduOYjx/jHwHOAR4P8BXyYbTxmvfwQmAg8DtwL1p6p+HHhVOlPqn9K4xvnAYmATWRfZ3wOd7J/3kp0ocD/wPeDDtdNmJZ2QjkROSHVfDNwu6Qmyz+s/gL/bz/3bIUC++ZHZgSHpy8AvI6L+CMHskOcjC7N9lLqAniKpJGkRcCHw9VbHZVYEnx9ttu+OJeuGeRLQA/xFRPy8tSGZFcPdUGZmNiZ3Q5mZ2ZgOm26oWbNmxbx581odhpnZIWXVqlUPR0TXWPUOm2Qxb948uru7Wx2GmdkhRdL946nnbigzMxtToclC0qI0jfJ6SZc1WP+xNK/P6jSD5aO5dRdLujs9fOcyM7MWKqwbKk2GdjVwHtlphSslLUtX3QIQEW/L1f9L4Mz0ujb1wUKy6RxWpbZbiorXzMyaK/LI4mxgfZqlsg+4nuyipWYuAr6UXr8UuCkiNqcEcROwqMBYzcxsFEUmi9nsOWVzTyobIc0eOh/4371ta2ZmxSsyWTSatrjZFYCLgRvSDWbG3VbSJZK6JXX39vbuY5hmZjaWIpNFD3vO7z+HbKbMRhazuwtq3G0j4pqIWBgRC7u6xjxN2MzM9lGRyWIlsEDZTe2rZAlhxG0nJZ0MzCCbZrpmBXB+ml9/Btm0zCuKCPLxXQN89Ka7WL3x0bErm5kdoQpLFhExQHYHsxXAncBXImKNpCslXZCrehFwfeQmqYqIzcAHyBLOSuDKVHbA9Q0M8U/fuZvbnCzMzJoq9AruiFhOdoOUfNkVdcvva9J2KbC0sOCSSjkbHukfHCp6V2Zmh6wj/gruSjn7EewacLIwM2vmiE8W1ZQsfGRhZtbcEZ8sSiXRUZKThZnZKI74ZAFZV1Sfu6HMzJpysiAb5O4f9B0DzcyacbIAqh1l+twNZWbWlJMFUC3L3VBmZqNwsgAqHSUPcJuZjcLJguz0WScLM7PmnCzw2VBmZmNxsiDrhurz2VBmZk05WQCd5RL9PrIwM2vKyQKodPgKbjOz0ThZkMYsnCzMzJpyssAD3GZmY3GyAKq+zsLMbFROFmTXWbgbysysOScL0kSCAz511sysmUKThaRFktZJWi/psiZ1Xi1praQ1kq7LlQ9KWp0ey4qM091QZmajK+we3JLKwNXAeUAPsFLSsohYm6uzALgcOCcitkg6OreJHRFxRlHx5XmA28xsdEUeWZwNrI+IDRHRB1wPXFhX58+AqyNiC0BEPFRgPE15zMLMbHRFJovZwMbcck8qyzsJOEnSLZJulbQot26CpO5U/opGO5B0SarT3dvbu8+BuhvKzGx0hXVDAWpQVj+K3AEsAM4F5gA/kHRaRDwKnBARmyQ9GfhfSb+IiHv22FjENcA1AAsXLtznEepKucRQwMDgEB1lj/mbmdUr8puxB5ibW54DbGpQ5xsR0R8R9wLryJIHEbEpPW8AbgbOLCrQSkoQvrWqmVljRSaLlcACSfMlVYHFQP1ZTV8HXgggaRZZt9QGSTMkdebKzwHWUpBqR/Zj8LiFmVljhXVDRcSApCXACqAMLI2INZKuBLojYllad76ktcAg8M6IeETS7wCfkTREltA+mD+L6kCrlrMeM49bmJk1VuSYBRGxHFheV3ZF7nUAb0+PfJ0fAU8vMra8WjeUT581M2vMo7nkxyycLMzMGnGyYPeYhZOFmVljThbsPrLY5W4oM7OGnCyAakdtgNunzpqZNeJkAVTLZcDdUGZmzThZkE1RDj4bysysGScLoOKL8szMRuVkQTbrLEC/jyzMzBpyssDTfZiZjcXJAl+UZ2Y2FicLchfl+T7cZmYNOVmQOxvKRxZmZg05WbB7gNunzpqZNeZkgccszMzG4mSBJxI0MxuLkwXQUfIV3GZmo3GyACRRLZfo80SCZmYNFZosJC2StE7SekmXNanzaklrJa2RdF2u/GJJd6fHxUXGCVlXlLuhzMwaK+y2qpLKwNXAeUAPsFLSsvy9tCUtAC4HzomILZKOTuUzgfcCC4EAVqW2W4qKt1KWu6HMzJoo8sjibGB9RGyIiD7geuDCujp/BlxdSwIR8VAqfylwU0RsTutuAhYVGCuVso8szMyaKTJZzAY25pZ7UlneScBJkm6RdKukRXvRFkmXSOqW1N3b27tfwVY7Sr4oz8ysiSKThRqU1Y8gdwALgHOBi4DPSZo+zrZExDURsTAiFnZ1de1XsNVyyd1QZmZNFJkseoC5ueU5wKYGdb4REf0RcS+wjix5jKftAeVuKDOz5opMFiuBBZLmS6oCi4FldXW+DrwQQNIssm6pDcAK4HxJMyTNAM5PZYWpdMj34DYza6Kws6EiYkDSErIv+TKwNCLWSLoS6I6IZexOCmuBQeCdEfEIgKQPkCUcgCsjYnNRsULWDeUjCzOzxgpLFgARsRxYXld2Re51AG9Pj/q2S4GlRcaXVymX2OUxCzOzhnwFd+KL8szMmnOySNwNZWbWnJNFUvGps2ZmTTlZJJWOks+GMjNrwski8UV5ZmbNOVkk1Q55ug8zsyacLBJfwW1m1pyTRVItl+h3N5SZWUNOFknFs86amTXlZJFk3VBBdlG5mZnlOVkk1XI2K7pPnzUzG8nJIql2ZD8KD3KbmY3kZJFUytmPwtdamJmN5GSR1JKFjyzMzEZyskhq3VA+I8rMbCQni6Tqbigzs6acLJLd3VA+G8rMrF6hyULSIknrJK2XdFmD9a+X1CtpdXr8aW7dYK68/t7dB5zPhjIza66w26pKKgNXA+cBPcBKScsiYm1d1S9HxJIGm9gREWcUFV+9SrrOwrdWNTMbqcgji7OB9RGxISL6gOuBCwvc336p+mwoM7OmikwWs4GNueWeVFbvlZJul3SDpLm58gmSuiXdKukVjXYg6ZJUp7u3t3e/gnU3lJlZc0UmCzUoqx89/iYwLyKeAXwb+EJu3QkRsRD4I+AfJT1lxMYiromIhRGxsKura7+C9UV5ZmbNFZkseoD8kcIcYFO+QkQ8EhG70uJngWfm1m1KzxuAm4EzC4zVF+WZmY2iyGSxElggab6kKrAY2OOsJknH5RYvAO5M5TMkdabXs4BzgPqB8QOq2pEdCPX51FkzsxEKOxsqIgYkLQFWAGVgaUSskXQl0B0Ry4A3S7oAGAA2A69PzZ8KfEbSEFlC+2CDs6gOqGq5DLgbysyskcKSBUBELAeW15VdkXt9OXB5g3Y/Ap5eZGz1Kh21KcqdLMzM6vkK7sRjFmZmzTlZJMMTCbobysxsBCeLZHgiQR9ZmJmN4GSRDHdDDfhsKDOzek4WSbkkyiV5zMLMrAEni5xKWe6GMjNrwMkip1IueYDbzKwBJ4uczo6Su6HMzBpwssjxkYWZWWNOFjmVso8szMwacbLIqZTle3CbmTXgZJFT7Sj7tqpmZg2MK1lIeoukqcr8i6SfSTq/6OAOtmrZ11mYmTUy3iOL/xMRjwHnA13AG4APFhZVi3jMwsyssfEmi9otUn8XuDYibqPxbVMPaVWfOmtm1tB4k8UqSf9DlixWSJoCHHbfqj511syssfHe/OiNwBnAhojYLmkmWVfUYaVSLvm2qmZmDYz3yOI5wLqIeFTSnwDvAbaO1UjSIknrJK2XdFmD9a+X1CtpdXr8aW7dxZLuTo+Lx/uG9oev4DYza2y8yeJTwHZJpwN/DdwP/OtoDSSVgauBlwGnAhdJOrVB1S9HxBnp8bnUdibwXuBZwNnAeyXNGGes+6xSlruhzMwaGG+yGIiIAC4EPh4RHwemjNHmbGB9RGyIiD7g+tR+PF4K3BQRmyNiC3ATsGicbfeZz4YyM2tsvMlim6TLgdcC/5WOGipjtJkNbMwt96Syeq+UdLukGyTN3Zu2ki6R1C2pu7e3d5xvpbmKu6HMzBoab7J4DbCL7HqLB8i+uD88RptGp9bWjx5/E5gXEc8Avg18YS/aEhHXRMTCiFjY1dU1Rjhjq5ZLvoLbzKyBcSWLlCC+CEyT9HJgZ0SMOmZBdjQwN7c8B9hUt91HImJXWvws8Mzxti2Cr7MwM2tsvNN9vBr4KfCHwKuBn0h61RjNVgILJM2XVAUWA8vqtntcbvEC4M70egVwvqQZaWD7/FRWKE8kaGbW2Hivs3g3cFZEPAQgqYus2+iGZg0iYkDSErIv+TKwNCLWSLoS6I6IZcCbJV0ADACbgdentpslfYAs4QBcGRGb9/rd7aVquczgUDA4FJRLh90F6mZm+2y8yaJUSxTJI4zjqCQilgPL68quyL2+HLi8SdulwNJxxndAVDqyBNE/OES5VD6YuzYza2vjTRbfkrQC+FJafg11SeBwUC1n+a9vcIgJFScLM7OacSWLiHinpFcC55CdqXRNRPxnoZG1QLUjSxb9PiPKzGwP4z2yICK+BnytwFharpI7sjAzs91GTRaSttHg+gayo4uIiKmFRNUitWTRP+AzoszM8kZNFhEx1pQeh5VaN5SPLMzM9uR7cOdUy9nZUJ5M0MxsT04WOcPdUD6yMDPbg5NFjpOFmVljThY5w2MW7oYyM9uDk0WOT501M2vMySKnOtwN5VNnzczynCxy3A1lZtaYk0VOpbx7IkEzM9vNySLHYxZmZo05WeR0dvjUWTOzRpwscoaPLDxmYWa2ByeLnIqPLMzMGnKyyPGps2ZmjRWaLCQtkrRO0npJl41S71WSQtLCtDxP0g5Jq9Pj00XGWVM7G2qXu6HMzPYw7psf7S1JZeBq4DygB1gpaVlErK2rNwV4M/CTuk3cExFnFBVfI5KolOVuKDOzOkUeWZwNrI+IDRHRB1wPXNig3geADwE7C4xl3Crlkm+ramZWp8hkMRvYmFvuSWXDJJ0JzI2IGxu0ny/p55K+J+l5jXYg6RJJ3ZK6e3t7D0jQ1Y6Sr7MwM6tTZLJQg7LhkWNJJeBjwDsa1PsNcEJEnAm8HbhO0ohbuEbENRGxMCIWdnV1HZCgK+WSu6HMzOoUmSx6gLm55TnAptzyFOA04GZJ9wHPBpZJWhgRuyLiEYCIWAXcA5xUYKzDquUSfb4Ht5nZHopMFiuBBZLmS6oCi4FltZURsTUiZkXEvIiYB9wKXBAR3ZK60gA5kp4MLAA2FBjrMHdDmZmNVNjZUBExIGkJsAIoA0sjYo2kK4HuiFg2SvPnA1dKGgAGgUsjYnNRseZVyvIAt5lZncKSBUBELAeW15Vd0aTuubnXXwO+VmRszXjMwsxsJF/BXcfdUGZmIzlZ1KmUS55I0MysjpNFnaq7oczMRnCyqFPtKHkiQTOzOk4WdSpluRvKzKyOk0Udnw1lZjaSk0WdatlnQ5mZ1XOyqFPt8NlQZmb1nCzquBvKzGwkJ4s6WbLw2VBmZnlOFnXcDWVmNpKTRZ1qWfQNDhHhowszsxonizqVcvYjGRhysjAzq3GyqFPtyH4k7ooyM9vNyaJO7cjCZ0SZme3mZFGnUjuycLIwMxvmZFGnc/jIwmMWZmY1hSYLSYskrZO0XtJlo9R7laSQtDBXdnlqt07SS4uMM6/SIcBjFmZmeYXdVlVSGbgaOA/oAVZKWhYRa+vqTQHeDPwkV3YqsBh4GnA88G1JJ0XEYFHx1njMwsxspCKPLM4G1kfEhojoA64HLmxQ7wPAh4CdubILgesjYldE3AusT9srXC1Z+MjCzGy3IpPFbGBjbrknlQ2TdCYwNyJu3Nu2qf0lkroldff29h6QoKse4DYzG6HIZKEGZcOjxpJKwMeAd+xt2+GCiGsiYmFELOzq6trnQPOqtW4oH1mYmQ0rbMyC7Ghgbm55DrAptzwFOA24WRLAscAySReMo21hOtORxfb+wodHzMwOGUUeWawEFkiaL6lKNmC9rLYyIrZGxKyImBcR84BbgQsiojvVWyypU9J8YAHw0wJjHfbkrqMAuPvBbQdjd2Zmh4TCjiwiYkDSEmAFUAaWRsQaSVcC3RGxbJS2ayR9BVgLDABvOhhnQgHMnFxl9vSJ3PHrxw7G7szMDglFdkMREcuB5XVlVzSpe27d8lXAVYUFN4qnHT+VOzZtbcWuzczakq/gbuC02dO49+EneHzXQKtDMTNrC04WDZw2eyoRsHaTu6LMzMDJoqHTjp8GwB2/dleUmRk4WTR09NQJHD2l08nCzCxxsmjitNnTPMhtZpY4WTRx2vFTWf/Q4+zo88V5ZmZOFk08bfY0hgLufMCD3GZmThZNPH22B7nNzGqcLJo4btoEZk6uOlmYmeFk0ZSk7EpuT/thZuZkMZrTZk/jrge3sWvAg9xmdmRzshjF02dPY2AoWPeAZ6A1syObk8Uodl/J7a4oMzuyOVmMYu7MiUyZ0OGL88zsiOdkMQpJnHb8NNb4jCgzO8I5WYzhtNlTufOBbfQP+p7cZnbkcrIYw1nzZtI3MMTyX/ym1aGYmbVMoclC0iJJ6yStl3RZg/WXSvqFpNWSfijp1FQ+T9KOVL5a0qeLjHM0L3nqMZxy7BQ+etNdProwsyNWYclCUhm4GngZcCpwUS0Z5FwXEU+PiDOADwEfza27JyLOSI9Li4pzLKWSeOdLT+b+R7bzle6NrQrDzKylijyyOBtYHxEbIqIPuB64MF8hIvLnpE4GosB49tmLTjmaZ544g3/6zt2ehdbMjkhFJovZQP5f8Z5UtgdJb5J0D9mRxZtzq+ZL+rmk70l6XqMdSLpEUrek7t7e3gMZe/1+eNeiU3jwsV184cf3FbYfM7N2VWSyUIOyEUcOEXF1RDwFeBfwnlT8G+CEiDgTeDtwnaSpDdpeExELI2JhV1fXAQx9pLPnz+Tck7v41M33sHVHf6H7MjNrN0Umix5gbm55DrBplPrXA68AiIhdEfFIer0KuAc4qaA4x+2vzj+ZrTv6+ez3N7Q6FDOzg6rIZLESWCBpvqQqsBhYlq8gaUFu8feAu1N5VxogR9KTgQVAy7+hT5s9jd8//Xiu+cEGvvvLh1odjpnZQVNYsoiIAWAJsAK4E/hKRKyRdKWkC1K1JZLWSFpN1t10cSp/PnC7pNuAG4BLI2JzUbHujSsveBonHXMUl/xbN//tay/M7AihiLY8AWmvLVy4MLq7uw/Kvrbu6OcN1/6U23q28pE/PJ1XnDli3N7M7JAgaVVELByrnq/g3gfTJlb4tzc+i7PnzeRtX1nNtbfcy9DQ4ZF0zcwacbLYR5M7O7j2DWfxwpOP5v3fXMsrPnkLq+7f0uqwzMwK4WSxHyZUynzudQv52GtO58HHdvLKT/2It17/czZu3t7q0MzMDqiOVgdwqCuVxB+cOYfzTz2WT918D9f8YAPfuG0Tz/2tWSw+6wRecurRdHaUWx2mmdl+8QD3Abbp0R18eeVGvtq9kU1bdzJjUoXzTz2WF5zcxTlPmcW0SZVWh2hmNmy8A9xOFgUZHAp+uP5hvtq9ke/d1cu2nQOUBGfMnc5Z82Zy+tzpnDF3OsdNm4DU6GJ3M7PijTdZuBuqIOWSeMFJXbzgpC4GBodYvfFRvn9XL9+/+2GuveU++tJ057OO6uTkY49iwdFTOOmYKfzW0Udx4pMm0XVUJ6WSk4iZtQcfWbTAroFBfvmbbdzW8yi392zl7ge3cfdDj7M9N6NtZ0eJE2ZOYs6MiRw3fSLHTZ3AcdMncszUTrqmdNJ1VCczJlWdUMxsv/jIoo11dpQ5fe50Tp87fbhsaCjYtHUH9/Q+wa8eeYJfbd7O/Y9sp2fLDm7r2crmJ/pGbKejJGZMrvKkyVVmTKoyc3KV6ZMqTJtYGX6eOqHClAkVpk7sYMqECpM7y0zprDChUnL3l5mNm5NFmyiVxJwZk5gzYxIwcgbdnf2DPLB1Jw9t20Xvtl30bsteb36ij81P9LFlex93PvAYW7f3s3VHPwNjXCRYUnatyORqB5M6y0yudjCxWmZSekyolJmYHhMqZSZUSkyolOmslJnQUaKzUqazo0S1o0RnelTLZaqprNpRolrOHpUOUS2XKJfkBGV2iHKyOERMqJSZN2sy82ZNHrNuRPBE3yCPbu9j284BHtvRz7adA2zb1c/juwZ5fOcAT+wa4PFdA2zvG+CJvkG278qeNz/Rx6+3DLK9b5Cd/dljR/8gB+oC9Wq5RKUsOmrPpSyZVEpZMqmVl0v5MtFRysrKpaxNeY9lUSrl6ih7LqXXpeEydpel8pKy8aVSKiuXsvuXZO2gJO1eVrauJFL72nJaR12dkhDZslIbUdsmu8ty65Ta19rVtlvLsfn1w9vJ7WP4daoLWXvS/urXK91JYI/tM3Jb+X3bkcnJ4jAkiaM6Oziq88B8vBFB3+AQuwaG2NU/xM7+QXYNDGbLA0P0DWRl/YNB38AQfYOD6TnoHxiibzCrMzA4RP9QVtafXg8MDtE/GAzs8XqIwaFgIL3e0R8MRbY8OJSVDQXZ8xD0Dw4xFLV12fPgUAyXeSaWA293Iqot1yUV9qzQNDnl2g8vN1s3WpsGsdF0X7m6Ters3tboyTGfZPPbGxnL2NsesSeNurhH+6ceN5V/vujMUWPdX04WNiZJdHaUs4sLJ7Q6mr0XkSWMWgLJJ5GhoWAwlQ0Nkb0eCiLI6kUMt6+1i6BuPXvUqS0Pr6NWvmfbLIllz7WySPFm7WpxpLKs+p7l7N5f1jZXt7actrl7eXdc9WXssX92l+Xb1LZV1z6/P+raskd51K1vvJ36z7C+7vC296g3cpv5kvx267cxYp/ULY9Yv+fORtaPXN2xttW8baP19QVzZ0ysr3HAOVnYYS/rRsq6m8xs33huKDMzG5OThZmZjcnJwszMxlRospC0SNI6SeslXdZg/aWSfiFptaQfSjo1t+7y1G6dpJcWGaeZmY2usGQhqQxcDbwMOBW4KJ8Mkusi4ukRcQbwIeCjqe2pwGLgacAi4JNpe2Zm1gJFHlmcDayPiA0R0QdcD1yYrxARj+UWJ7P7hLALgesjYldE3AusT9szM7MWKPLU2dnAxtxyD/Cs+kqS3gS8HagCL8q1vbWu7ewGbS8BLgE44YQTDkjQZmY2UpFHFo1Oah95bUnE1RHxFOBdwHv2su01EbEwIhZ2dY2cT8nMzA6MIo8seoC5ueU5wKZR6l8PfGof27Jq1aqHJd2/D3HWzAIe3o/2RWnXuKB9Y2vXuKB9Y2vXuKB9Y2vXuGDvYjtxPJWKTBYrgQWS5gO/Jhuw/qN8BUkLIuLutPh7QO31MuA6SR8FjgcWAD8dbWcRsV+HFpK6xzOn+8HWrnFB+8bWrnFB+8bWrnFB+8bWrnFBMbEVliwiYkDSEmAFUAaWRsQaSVcC3RGxDFgi6SVAP7AFuDi1XSPpK8BaYAB4U0QMNtyRmZkVrtC5oSJiObC8ruyK3Ou3jNL2KuCq4qIzM7Px8hXcu13T6gCaaNe4oH1ja9e4oH1ja9e4oH1ja9e4oIDYDpt7cJuZWXF8ZGFmZmNysjAzszEd8clirMkOD3IsSyU9JOmOXNlMSTdJujs9z2hBXHMlfVcbS+hOAAAHfklEQVTSnZLWSHpLG8U2QdJPJd2WYnt/Kp8v6Scpti9Lqh7s2FIcZUk/l3Rjm8V1X24Sz+5U1g6f53RJN0j6Zfp9e06bxHVy+lnVHo9JemubxPa29Lt/h6Qvpb+JA/57dkQni3FOdngwfZ5s4sS8y4DvRMQC4Dtp+WAbAN4REU8Fng28Kf2c2iG2XcCLIuJ04AxgkaRnA38PfCzFtgV4YwtiA3gLcGduuV3iAnhhRJyROx+/HT7PjwPfiohTgNPJfnYtjysi1qWf1RnAM4HtwH+2OjZJs4E3Awsj4jSyyxQWU8TvWQzfG/jIewDPAVbkli8HLm9xTPOAO3LL64Dj0uvjgHVt8HP7BnBeu8UGTAJ+RjYH2cNAR6PP+SDGM4fsC+RFwI1k09i0PK607/uAWXVlLf08ganAvaQTb9olrgZxng/c0g6xsXsOvplkl0LcCLy0iN+zI/rIgsaTHY6YsLDFjomI3wCk56NbGYykecCZwE9ok9hSV89q4CHgJuAe4NGIGEhVWvW5/iPw18BQWn5Sm8QF2Vxr/yNpVZqQE1r/eT4Z6AWuTV13n5M0uQ3iqrcY+FJ63dLYIuLXwD8AvwJ+A2wFVlHA79mRnizGNWGhZSQdBXwNeGvsOb18S0XEYGTdA3PIprJ/aqNqBzMmSS8HHoqIVfniBlVb9ft2TkT8NlkX7JskPb9FceR1AL8NfCoizgSeoDVdYU2lvv8LgK+2OhaANEZyITCfbGqkyWSfab39/j070pPFXk9Y2AIPSjoOID0/1IogJFXIEsUXI+I/2im2moh4FLiZbFxluqTaDAWt+FzPAS6QdB/ZJJkvIjvSaHVcAETEpvT8EFnf+9m0/vPsAXoi4idp+Qay5NHquPJeBvwsIh5My62O7SXAvRHRGxH9wH8Av0MBv2dHerIYnuww/cewmGwSw3ayjDRnVnr+xsEOQJKAfwHujIiPtllsXZKmp9cTyf547gS+C7yqVbFFxOURMSci5pH9Xv1vRPxxq+MCkDRZ0pTaa7I++Dto8ecZEQ8AGyWdnIpeTDY/XMt/z3IuYncXFLQ+tl8Bz5Y0Kf2d1n5mB/73rJUDRe3wAH4XuIusn/vdLY7lS2T9jv1k/2W9kayf+ztkM/J+B5jZgrieS3YYezuwOj1+t01iewbw8xTbHcAVqfzJZDMVryfrMuhs4ed6LnBju8SVYrgtPdbUfu/b5PM8A+hOn+fXgRntEFeKbRLwCDAtV9by2ID3A79Mv///BnQW8Xvm6T7MzGxMR3o3lJmZjYOThZmZjcnJwszMxuRkYWZmY3KyMDOzMTlZ2CFF0o/S8zxJf3SAt/03jfZVFEmvkHTF2DX3adt/M3atUdsvkfSGAxWPHfp86qwdkiSdC/xVRLx8L9qUI2JwlPWPR8RRByK+ccbzI+CCiHh4P7cz4n3t73uRNIlssrwz9yc2O3z4yMIOKZIeTy8/CDwv3VvgbWkywQ9LWinpdkl/nuqfq+xeHNcBv0hlX08T6K2pTaIn6YPAxLS9L+b3pcyH0/0CfiHpNblt35y7/8IX01W0SPqgpLUpln9o8D5OAnbVEoWkz0v6tKQfSLorzS1VmyRxXO8rt+1G7+VPlN33Y7Wkz6Tp+ZH0uKSrlN0P5FZJxwBExHbgPklnH4jPzQ4DrbgS0g8/9vUBPJ6ezyVdFZ2WLwHek153kl0FPD/VewKYn6s7Mz1PJLvq9Un5bTfY1yvJZrMtA8eQTbFwXNr2VrK5d0rAj8mudp9JNnV17ch9eoP38QbgI7nlzwPfSttZQHYF/4S9eV+NYk+vnwp8E6ik5U8Cr0uvA/j99PpDtX2l5XeT3cek5Z+7H61/1CaaMjvUnQ88Q1JtPpxpZF+6fcBPI+LeXN03S/qD9HpuqvfIKNt+LvClyLp6HpT0PeAs4LG07R6ANE36POBWYCfwOUn/RXaPgXrHkU3HnfeViBgC7pa0AThlL99XMy8mu2HPynTgM5HdE9715eJbRXafkpqHUgxmThZ22BDwlxGxYo/CbGzjibrllwDPiYjtkm4m+w9+rG03syv3epDshjMDqfvmxWSTCC4hm3U2bwfZF39e/QBiMM73NQYBX4iIyxus64+I2n4H2fM7YUKK08xjFnbI2gZMyS2vAP4iTaWOpJPSjKr1pgFbUqI4hWw685r+Wvs63wdek8YPuoDnk03S1pCy+35Mi4jlwFvJJserdyfwW3VlfyipJOkpZBPBrduL91Uv/16+A7xK0tFpGzMlnTiObZxE1k1n5iMLO2TdDgxIuo2sv//jZF1AP0uDzL3AKxq0+xZwqaTbyb6Mb82tuwa4XdLPIptOvOY/yW5NeRvZf/t/HREPpGTTyBTgG5ImkP1X/7YGdb4PfESScv/ZrwO+RzYucmlE7JT0uXG+r3p7vBdJ7yG7M16JbFbjNwH3j7GNc8hmNDXzqbNmrSLp48A3I+Lbkj5PNmB/Q4vDAkDSmcDbI+K1rY7F2oO7ocxa5+/I7pHQjmYBf9vqIKx9+MjCzMzG5CMLMzMbk5OFmZmNycnCzMzG5GRhZmZjcrIwM7Mx/X+o961HAkua/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 利用matplotlib來畫學習曲線 (損失)，y軸是我們的損失，x軸是訓練的次數。\n",
    "loss = np.squeeze(d['loss']) \n",
    "plt.plot(loss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (per ten)') \n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"])) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 利用matplotlib來畫學習曲線 (accuracy)。\n",
    "accuarcy = np.squeeze(d['accuarcy']) \n",
    "plt.plot(accuarcy)\n",
    "plt.ylabel('accuarcy')\n",
    "plt.xlabel('iterations (per ten)') \n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"])) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test phase \n",
    "最後，我們拿測試資料的預測值跟實際值做比較，準確率大約為0.85。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8526503286038941\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \", np.mean(d['Y_prediction_test']== test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
