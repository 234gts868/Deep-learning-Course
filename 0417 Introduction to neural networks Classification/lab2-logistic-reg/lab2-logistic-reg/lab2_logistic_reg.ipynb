{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 : Logistic Regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "在這個lab會實作簡單的 logistic regression，將它應用在分類的模型，利用training data 訓練完成後，再利用testing data來檢驗模型的好壞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此提供原生資料train.csv。我們的任務就是利用資料裡的欄位當成特徵值，搭配 logistic regression的計算流程去預測income，income大於50K為1，小於等於50K為0，最後利用測試資料來測準確率。<br>已經完成feature format的檔案為X_train.csv、Y_train.csv、X_test.csv、Y_test.csv。\n",
    "\n",
    "<br>訓練資料是用來訓練模型的，而測試資料則是來評估訓練後模型的準確率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.csv : 原生訓練資料<br>X_train.csv: 處理後的訓練資料X，有32561筆資料，每筆資料的特徵維度(數)是106。<br>Y_train.csv: 處理後的訓練資料Ｙ，有32561筆資料，每筆資料有一個標籤。<br>X_test.csv: 處理後的測試資料Ｘ，有16281筆資料，每筆資料的特徵維度(數)是106。<br>Y_test.csv: 處理後的測試資料Ｙ，有16281筆資料，每筆資料代表一個標籤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料欄位\n",
    "- income: >50K, <=50K\n",
    "- age: continuous.\n",
    "- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "- fnlwgt: continuous.\n",
    "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "- education-num: continuous.\n",
    "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "- sex: Female, Male.\n",
    "- capital-gain: continuous.\n",
    "- capital-loss: continuous.\n",
    "- hours-per-week: continuous.\n",
    "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先匯入模組去檢驗資料，我們會利用這些模組的函式幫忙取資料、做運算和畫圖等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "這邊會得到各兩個訓練資料跟測試資料，每個列代表一筆資料，所以資料的總數 = 列的數，利用資料的特徵去預測income。\n",
    "\n",
    "- train_x的維度是(32561, 106)，32561為資料數，106為特徵數。\n",
    "- train_y的維度是(32561, 1)，32561為資料數，1為標籤數。\n",
    "- test_x的維度是(16281, 106)，16281為資料數，106為特徵數。\n",
    "- test_y的維度是(16281, 1)，16281為資料數，1為標籤數。\n",
    "\n",
    "特徵 :資料的描述，如資料欄位中的age、workclass...。<br>\n",
    "標籤 :為每筆資料要預測的目標或者正確答案，資料有多少筆就代表有多少標籤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 data_path，依據放資料的資料夾去設。\n",
    "dir_data = './data/'\n",
    "\n",
    "# 讀取檔案\n",
    "data = pd.read_csv(dir_data+'X_train.csv')\n",
    "train_x = np.array(pd.read_csv(dir_data+'X_train.csv'))\n",
    "train_y = np.array(pd.read_csv(dir_data+'Y_train.csv'))\n",
    "test_x = np.array(pd.read_csv(dir_data+'X_test.csv'))\n",
    "test_y = np.array(pd.read_csv(dir_data+'Y_test.csv'))\n",
    "# 檢視我們的資料，這邊先呈現5筆，他會有106個欄位，也就是特徵值。\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know how many training and test samples\n",
    "\n",
    "許多在深度學習的錯誤來自於矩陣或向量的維度不相符。如果能了解資料的維度，可降低錯誤的發生。<br>\n",
    "寫出下方的程式碼，可得知資料維度。\n",
    "\n",
    "```\n",
    "num_train : 訓練資料的數量\n",
    "num_test : 測試資料的數量\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得長度可知道有幾筆。\n",
    "\n",
    "# Your code here\n",
    "num_train = \n",
    "num_test = \n",
    "# End your code\n",
    "\n",
    "# 看訓練資料跟測試資料的維度 (資料數 , 特徵數)。\n",
    "print(\"訓練資料的數目: num_train = \" + str(num_train)) # 32561\n",
    "print(\"測試資料的數量: num_test = \" + str(num_test)) # 16281\n",
    "print(\"train_x的維度: \" + str(train_x.shape)) # (32561, 106)\n",
    "print(\"train_y的維度: \" + str(train_y.shape)) # (32561, 1)\n",
    "print(\"test_x的維度: \" + str(test_x.shape)) # (16281, 106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dimensions\n",
    "改變訓練跟測試資料的維度變成 (特徵數，資料數)，讓資料能夠做矩陣運算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊使用轉置函數T去讓去讓維度做交換。\n",
    "# Your code here \n",
    "train_x = \n",
    "test_x =\n",
    "train_y =\n",
    "test_y = \n",
    "# End your code\n",
    "\n",
    "print(\"\\n改變後的維度\")\n",
    "print(\"train_x : \" + str(train_x.shape))# (106, 32561)\n",
    "print(\"train_y : \" + str(train_y.shape))# (1, 32561)\n",
    "print(\"test_x  : \" + str(test_x.shape))# (106, 16281)\n",
    "print(\"test_y  : \" + str(test_y.shape))# (1, 16281)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalization (feature scaling)\n",
    "\n",
    "特徵間的範圍會不同，所以利用特徵標準化去讓範圍一致以便更快收斂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用平均數跟標準差的公式算出mu跟sigma。\n",
    "def compute_mean_std(X):\n",
    "    \"\"\" \n",
    "    X: 訓練資料\n",
    "    mu: 平均數\n",
    "    sigma: 標準差\n",
    "    \"\"\"  \n",
    "    # Your code here\n",
    "    # hint :若利用np.mean()，需加上`keepdim=True`。\n",
    "    mu = \n",
    "    sigma = \n",
    "    # End your code\n",
    "    return mu, sigma\n",
    "\n",
    "# 利用平均數跟標準差進行標準化。\n",
    "def normalize_feat(X, mu, sigma):\n",
    "    \n",
    "    # Your code here\n",
    "    # 寫出標準化的公式\n",
    "    normalized_X = \n",
    "    # End your code\n",
    "    return normalized_X\n",
    "\n",
    "# Your code here\n",
    "# 用 train_x 計算 mu, sigma\n",
    "mu, sigma = \n",
    "# End your code\n",
    "\n",
    "print('前五個特徵的平均值 : \\n', mu[:5])# [[3.85816468e+01] [1.89778367e+05][6.69205491e-01][1.07764884e+03][8.73038297e+01]]\n",
    "print('前五個特徵的平均值和標準差 : \\n', sigma[:5])# [[1.36402231e+01][1.05548357e+05][4.70499205e-01][7.38517868e+03][4.02954031e+02]]\n",
    "\n",
    "# 讓訓練資料跟測試資料都進行標準化。\n",
    "# 測試資料的標準化的參數需跟訓練資料一樣，不然會影響預測結果。\n",
    "train_x = normalize_feat(train_x, mu, sigma)\n",
    "test_x = normalize_feat(test_x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing parameters \n",
    "在下方的cell進行參數初始化。我們這個lab的參數會是w跟b，w會是一個矩陣，可以利用np.zeros()來實作，而b是一個純量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \n",
    "    \"\"\"\n",
    "    這個函式會建立一個(dim, 1)維度的w，然後b會初始化為0。\n",
    "    dim -- w的維度大小，也是特徵的數量。\n",
    "    w -- (dim, 1)\n",
    "    b -- 純量\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # hint : w可用np.zeros()做初始化\n",
    "    w = \n",
    "    b = \n",
    "    # End your code\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# 初始化參數\n",
    "dim = train_x.shape[0]\n",
    "w, b = initialize_with_zeros(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Get the cost and gradients\n",
    "利用損失函數跟梯度下降來幫助訓練參數。\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "我們會利用propagate function:\n",
    "\n",
    "我們利用正向傳播 (Forward propagation)去算資料X的損失:\n",
    "- 會有資料X，X為二維矩陣\n",
    "- 去計算 : $Z = w^T X + b\\quad(1)$\n",
    "<br><br>\n",
    "- 然後利用sigmoid函數去算預測值。\n",
    "$$A = \\sigma(Z) = \\frac{1}{1 + e^{-Z}}\\quad(2)$$    \n",
    "<br><br>\n",
    "- 利用損失函數去算預測值與實際值的落差。 $$L =-\\frac{1}{N} [\\sum_{n=1}^N y^nlog(a^n) + (1-y^n)log(1-a^n)]\\quad(3)$$\n",
    "\n",
    "更新參數的部分，可以使用以下的式子得到梯度: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{N}X(A-Y)^T\\quad(4)$$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^N (a^{(n)}-y^{(n)})\\quad(5)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求損失\n",
    "def compute_loss(A, Y) :\n",
    "    '''\n",
    "    A -- 預測值 (1, 訓練資料數)\n",
    "    Y -- 實際值 (1, 訓練資料數)\n",
    "    '''\n",
    "    # 參考公式 (3)。\n",
    "    # Your code here\n",
    "    loss = \n",
    "    # end your code\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 帶入公式 (2)，範圍會被限制在0~1之間。\n",
    "def sigmoid(z) :\n",
    "    # Your code here\n",
    "    s = \n",
    "    # End your code\n",
    "    return s\n",
    "\n",
    "# 算出參數 (w, b)的梯度。\n",
    "def propagate(w, b, X, Y):\n",
    "    \n",
    "    \"\"\"\n",
    "    w -- (特徵數, 1)\n",
    "    b -- 一個純量\n",
    "    X -- data of size (特徵數, 資料數)\n",
    "    Y -- 實際值 (1, 資料數)\n",
    "    \n",
    "    loss -- 預測值與實際值的落差。\n",
    "    dw -- w的梯度，維度跟w一樣。\n",
    "    db -- b的低度，維度跟b一樣。\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    N = X.shape[1]\n",
    "    \n",
    "    # 正向傳播，利用公式 (1)和公式 (2)求得預測值A，再利用A與Y (實際值)來求損失。\n",
    "    \n",
    "    # Your code here\n",
    "    # 利用公式 (1)算預測值。\n",
    "    Z = \n",
    "    \n",
    "    # 利用公式 (2)讓預測值範圍限制在0到1之間。\n",
    "    A = \n",
    "    # End your code\n",
    "    \n",
    "    # 限制A的範圍。\n",
    "    A = np.clip(A, 1e-6, 1-1e-6)\n",
    "    \n",
    "    # 呼叫 compute_loss()，利用預測值與實際值去算損失\n",
    "    # Your code here\n",
    "    loss = \n",
    "    # End your code\n",
    "    \n",
    "    # 反向傳播，這邊會算出參數的梯度，我們之後利用此梯度來更新參數並讓損失函數最小化。\n",
    "    # Your code here\n",
    "    dw = \n",
    "    db = \n",
    "    # End your code\n",
    "\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    loss = np.squeeze(loss)\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    # 把參數的梯度存起來，之後會用來做更新。\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "利用公式(1)和公式(2)算出預測值，因為我們是要做0/1的分類，所以會再對每一筆預測值依是否大於0.5轉成0或1，這樣就可以跟實際值做比較。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算預測值\n",
    "def predict(w, b, X):\n",
    "    \n",
    "    '''\n",
    "    w -- (特徵數, 1)\n",
    "    b -- 純量\n",
    "    X -- (特徵數, 資料數)\n",
    "    Y_prediction -- 為一個numpy矩陣，包含對X的預測值(0/1)。\n",
    "    '''\n",
    "    # 設一陣列存預測值。\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    \n",
    "    # Your code here\n",
    "    # 利用公式 (1)算預測值。\n",
    "    Z = \n",
    "    # 利用公式 (2)讓預測值範圍限制在0到1之間。\n",
    "    A = \n",
    "    # End your code\n",
    "    \n",
    "\n",
    "    # 預測值若大於0.5為1，反之為0，可用 np.where()來分類。\n",
    "    Y_prediction[0, :A.shape[1]] = np.where(A[0, :A.shape[1]] > 0.5, 1, 0)\n",
    "        \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "- 計算損失跟梯度。\n",
    "- 利用參數的梯度來更新參數。\n",
    "\n",
    "目的是最小化損失函數 $L$來學習$w$ 和 $b$. 以參數 $\\theta$來說, 更新規則為$ \\theta = \\theta - \\eta \\text{ } d\\theta$,\n",
    "其中$\\eta$ 為學習率，用來調整更新的幅度。\n",
    "\n",
    "我們會紀錄每次更新參數後對損失的影響。<br>\n",
    "另外，也會呼叫predict()得到每筆資料預測值，算出每次更新參數後的準確率，並記錄起來。\n",
    "我們記錄的損失跟準確率會以學習曲線的方式呈現，可看出藉由參數的更新是否會讓模型的預測效果更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新參數\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_loss = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    此函數會藉由梯度下降函式來更新w跟b。\n",
    "    \n",
    "    w -- (特徵數, 1)\n",
    "    b -- 一個純量\n",
    "    X -- data of size (特徵數, 資料數)\n",
    "    Y -- 實際值 (1, 資料數)\n",
    "    num_iterations -- 要更新的次數。\n",
    "    learning_rate -- 梯度下降的學習率。\n",
    "    print_loss -- 當True，則印出損失。\n",
    "    Y_prediction -- 為一個numpy矩陣，包含對X的預測值(0/1)。\n",
    "    \n",
    "    Returns:\n",
    "    params -- 為一個dictionary (資料結構)，裡面包含w,b。\n",
    "    grads -- 為一個dictionary，裡面包含dw, db。\n",
    "    losses -- 為一個list，包含所有更新參數時紀錄的損失，會用來畫學習曲線。\n",
    "    \n",
    "    Tips:\n",
    "        1) 利用propagate()去計算目前參數的損失跟梯度。\n",
    "        2) 利用梯度下降的公式去更新w跟b。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 用來存每次損失的值，目的是看出損失的變化。\n",
    "    losses = []\n",
    "    # 紀錄每筆資料的accuracy。\n",
    "    accuracy_temp = 0\n",
    "    # 用來存每次accuracy的值，目的是看出accuracy的變化。\n",
    "    accuracy = []\n",
    " \n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # 呼叫propagate()計算梯度跟損失函數的值。\n",
    "        # Your code here\n",
    "        grads, loss = \n",
    "        # End your code\n",
    "        \n",
    "        \n",
    "        # 得到各自的梯度。\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # 更新參數\n",
    "        # Your code here\n",
    "        w = \n",
    "        b = \n",
    "        # End your code\n",
    "        # 紀錄每10次epoch的損失跟準確率，epoch就是訓練的次數。\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            \n",
    "            # 會把每次的損失加進losses。\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Your code here\n",
    "            # 呼叫predict()，算出資料x的預測值。\n",
    "            Y_prediction = \n",
    "            # End your code\n",
    "            \n",
    "            # 算準確率 : 利用求得的Y_prediction跟我們的實際值去做比較，這邊是算兩者有相同數量，再除全部的資料數。\n",
    "            accuracy_temp = np.mean(Y_prediction == Y)\n",
    "            \n",
    "            accuracy.append(accuracy_temp)\n",
    "            \n",
    "        # 印出每100次epoch的損失狀況。\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, loss))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, losses , accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all functions into a model\n",
    "我們將前面的函式整合起來，跑一次 logistic regression的流程。\n",
    "<br>初始化 -> 算出梯度跟損失 -> 更新參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_x, train_y, test_x, test_y, num_iterations, learning_rate, print_loss=False):\n",
    "    \"\"\"\n",
    "    這邊建立一個 logistic regression的模型來呼叫前面的函式。\n",
    "    \n",
    "    X_train -- (特徵數, 訓練資料數)\n",
    "    Y_train -- (1, 訓練資料數)\n",
    "    X_test --  (特徵數, 測試資料數)\n",
    "    Y_test --  (1, 測試資料數)\n",
    "    num_iterations -- 要更新的次數。\n",
    "    learning_rate -- 梯度下降的學習率。\n",
    "    print_loss -- 當True，則印出損失。\n",
    "    Y_prediction -- 為一個numpy矩陣，包含對X的預測值(0/1)。\n",
    "    \n",
    "    d -- 為一個dictionary，包含model的資訊。\n",
    "    \"\"\"\n",
    "    # 使用initialize_with_zeros()來初始化參數，w跟b。\n",
    "    # Your code here\n",
    "    w, b = \n",
    "    # end your code\n",
    "    \n",
    "    # 利用optimize()算出損失跟準確率，且算出梯度去更新參數。\n",
    "    # Your code here\n",
    "    params, grads, losses, accuracy = \n",
    "    # end your code\n",
    "    \n",
    "    # 從params中取得w跟b。\n",
    "    # Your code here\n",
    "    w = \n",
    "    b = \n",
    "    # end your code\n",
    "    \n",
    "    # \"訓練完\"模型後的預測值。\n",
    "    Y_prediction_train = predict(w, b, train_x)\n",
    "    Y_prediction_test = predict(w, b, test_x)\n",
    "    \n",
    "    # 印出訓練的損失。\n",
    "    print(\"train loss: {}\".format(losses[-1]))\n",
    "\n",
    "    # 將所有的變數全存起來。\n",
    "    d = {\"loss\": losses,\n",
    "         \"accuarcy\" : accuracy,\n",
    "         \"Y_prediction_test\": Y_prediction_test,\n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(train_x, train_y, test_x, test_y, num_iterations=800, learning_rate=0.5, print_loss=True) \n",
    "'''\n",
    "Cost after iteration 0: 0.693147\n",
    "Cost after iteration 100: 0.323456\n",
    "Cost after iteration 200: 0.318884\n",
    "Cost after iteration 300: 0.317393\n",
    "Cost after iteration 400: 0.316733\n",
    "Cost after iteration 500: 0.316395\n",
    "Cost after iteration 600: 0.316205\n",
    "Cost after iteration 700: 0.316090\n",
    "train loss: 0.3160215353350179\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用matplotlib來畫學習曲線 (損失)，y軸是我們的損失，x軸是訓練的次數。\n",
    "loss = np.squeeze(d['loss']) \n",
    "plt.plot(loss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (per ten)') \n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"])) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用matplotlib來畫學習曲線 (accuracy)。\n",
    "accuarcy = np.squeeze(d['accuarcy']) \n",
    "plt.plot(accuarcy)\n",
    "plt.ylabel('accuarcy')\n",
    "plt.xlabel('iterations (per ten)') \n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"])) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test phase \n",
    "最後，我們拿測試資料的預測值跟實際值做比較，準確率大約為0.85。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy : \", np.mean(d['Y_prediction_test']== test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
